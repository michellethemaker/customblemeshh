{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72dde6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "from contextlib import closing\n",
    "\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from gym import utils\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\n",
    "MAPS = {\n",
    "    \"4x4\": [\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],\n",
    "    \"8x8\": [\n",
    "        \"NFFFFFFF\",\n",
    "        \"FFFFFFFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FFFFFHFF\",\n",
    "        \"FFFHFFFF\",\n",
    "        \"FHHFFFHF\",\n",
    "        \"FHFFHFHF\",\n",
    "        \"FFFHFFFG\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def generate_random_map(size=8, p=0.8):\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is frozen\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((0, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        res[0][0] = \"S\"\n",
    "        res[-1][-1] = \"G\"\n",
    "        valid = is_valid(res)\n",
    "    return [\"\".join(x) for x in res]\n",
    "    \n",
    "    \n",
    "\n",
    "class customblemeshEnv(gym.Env):\n",
    "     \"\"\"\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "metadata = {\"render.modes\": [\"human\", \"ansi\"]}\n",
    "\n",
    "def __init__(self, desc=None, map_name=\"4x4\", is_slippery=True):\n",
    "    if desc is None and map_name is None:\n",
    "        desc = generate_random_map()\n",
    "    elif desc is None:\n",
    "        desc = MAPS[map_name]\n",
    "    self.desc = desc = np.asarray(desc, dtype=\"c\")\n",
    "    self.nrow, self.ncol = nrow, ncol = desc.shape\n",
    "    self.reward_range = (0, 1)\n",
    "\n",
    "    nA = 4\n",
    "    nS = nrow * ncol\n",
    "\n",
    "    isd = np.array(desc == b\"S\").astype(\"float64\").ravel()\n",
    "    isd /= isd.sum()\n",
    "\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "\n",
    "    def to_s(row, col):\n",
    "        return row * ncol + col\n",
    "\n",
    "    def inc(row, col, a):\n",
    "        if a == LEFT:\n",
    "            col = max(col - 1, 0)\n",
    "        elif a == DOWN:\n",
    "            row = min(row + 1, nrow - 1)\n",
    "        elif a == RIGHT:\n",
    "            col = min(col + 1, ncol - 1)\n",
    "        elif a == UP:\n",
    "            row = max(row - 1, 0)\n",
    "        return (row, col)\n",
    "\n",
    "    def update_probability_matrix(row, col, action):\n",
    "        newrow, newcol = inc(row, col, action)\n",
    "        newstate = to_s(newrow, newcol)\n",
    "        newletter = desc[newrow, newcol]\n",
    "        done = bytes(newletter) in b\"GH\"\n",
    "        reward = float(newletter == b\"G\")\n",
    "        return newstate, reward, done\n",
    "\n",
    "    for row in range(nrow):\n",
    "        for col in range(ncol):\n",
    "            s = to_s(row, col)\n",
    "            for a in range(4):\n",
    "                li = P[s][a]\n",
    "                letter = desc[row, col]\n",
    "                if letter in b\"GH\":\n",
    "                    li.append((1.0, s, 0, True))\n",
    "                else:\n",
    "                    if is_slippery:\n",
    "                        for b in [(a - 1) % 4, a, (a + 1) % 4]:\n",
    "                            li.append(\n",
    "                                (1.0 / 3.0, *update_probability_matrix(row, col, b))\n",
    "                            )\n",
    "                    else:\n",
    "                        li.append((1.0, *update_probability_matrix(row, col, a)))\n",
    "\n",
    "    super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "def render(self, mode=\"human\"):\n",
    "    outfile = StringIO() if mode == \"ansi\" else sys.stdout\n",
    "\n",
    "    row, col = self.s // self.ncol, self.s % self.ncol\n",
    "    desc = self.desc.tolist()\n",
    "    desc = [[c.decode(\"utf-8\") for c in line] for line in desc]\n",
    "    desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
    "    if self.lastaction is not None:\n",
    "        outfile.write(\n",
    "            \"  ({})\\n\".format([\"Left\", \"Down\", \"Right\", \"Up\"][self.lastaction])\n",
    "        )\n",
    "    else:\n",
    "        outfile.write(\"\\n\")\n",
    "    outfile.write(\"\\n\".join(\"\".join(line) for line in desc) + \"\\n\")\n",
    "\n",
    "    if mode != \"human\":\n",
    "        with closing(outfile):\n",
    "            return outfile.getvalue()\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d94bdb28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9de2ef14718d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Q-learning algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# initialize new episode params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_episodes' is not defined"
     ]
    }
   ],
   "source": [
    "# create this list to hold all of the rewards we'll get from each episode.\n",
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) \n",
    "        else:\n",
    "            action = env.action_space.sample()        \n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "    \n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "        learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward \n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print updated Q-table\n",
    "print(\"\\n\\n********Q-table********\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868496be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-108347fbf872>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"*****EPISODE \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*****\\n\\n\\n\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "\n",
    "        # Set new state\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4660882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
